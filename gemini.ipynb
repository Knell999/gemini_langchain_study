{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09a1efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY가 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# API 키 확인 (선택 사항)\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"GOOGLE_API_KEY가 설정되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY가 설정되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d7ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini 모델이 성공적으로 초기화되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini 모델 초기화\n",
    "# model_name은 \"gemini-pro\" 또는 \"gemini-1.5-pro-latest\" 등을 사용할 수 있습니다.\n",
    "# temperature는 생성될 텍스트의 무작위성(창의성)을 조절합니다. 0에 가까울수록 보수적입니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "\n",
    "print(\"Gemini 모델이 성공적으로 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d828f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 한국의 수도는 어디인가요?\n",
      "답변: 대한민국의 수도는 **서울**입니다.\n",
      "\n",
      "질문: 한국의 수도 서울에 대해 50자 이내로 설명해주세요.\n",
      "답변: 한국의 수도 서울은 역사와 현대가 공존하는 활기찬 대도시입니다. (28자)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 단일 질문\n",
    "prompt = \"한국의 수도는 어디인가요?\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(f\"질문: {prompt}\")\n",
    "print(f\"답변: {response.content}\")\n",
    "\n",
    "# 조금 더 복잡한 질문\n",
    "prompt_complex = \"한국의 수도 서울에 대해 50자 이내로 설명해주세요.\"\n",
    "response_complex = llm.invoke(prompt_complex)\n",
    "print(f\"\\n질문: {prompt_complex}\")\n",
    "print(f\"답변: {response_complex.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a023e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 기록:\n",
      "  human: 안녕하세요! 저는 AI 튜터입니다. 무엇을 도와드릴까요?\n",
      "  ai: 네, 반갑습니다! 저는 대한민국에 대해 궁금한 점이 많아요.\n",
      "  human: 무엇이든 물어보세요. 예를 들어, 수도나 문화에 대해 궁금한가요?\n",
      "  새로운 질문: 대한민국의 상징적인 동물은 무엇인가요?\n",
      "Gemini 답변: 네, 아주 좋은 질문입니다!\n",
      "\n",
      "대한민국의 상징적인 동물은 바로 **호랑이**입니다.\n",
      "\n",
      "특히 **백두산 호랑이**는 한국인의 기상과 용맹함을 상징하며, 예로부터 한반도의 지형을 호랑이 모양으로 비유하기도 했습니다. 한국 민화나 설화에도 호랑이가 자주 등장하며, 산신령의 사자(使者)로 여겨지기도 하는 등 한국 문화와 깊이 연관되어 있습니다.\n",
      "\n",
      "2018 평창 동계올림픽 마스코트인 '수호랑'도 호랑이를 모티브로 한 것이었죠.\n",
      "\n",
      "궁금증이 해결되셨기를 바랍니다! 또 다른 질문이 있으신가요?\n",
      "\n",
      "추가 질문: 그 동물이 어떤 특징을 가지고 있나요?\n",
      "Gemini 답변: 네, 대한민국의 상징적인 동물인 **호랑이**, 특히 **백두산 호랑이 (시베리아 호랑이)**는 다음과 같은 특징들을 가지고 있습니다.\n",
      "\n",
      "1.  **가장 큰 호랑이 아종:** 시베리아 호랑이는 현존하는 호랑이 아종 중에서 몸집이 가장 큽니다. 수컷은 몸길이 2.5~3미터(꼬리 포함), 몸무게 200~300kg에 달하기도 합니다. 암컷은 그보다 작습니다.\n",
      "\n",
      "2.  **두껍고 긴 털:** 추운 북방 지역에 서식하기 때문에 다른 호랑이 아종에 비해 털이 길고 빽빽하며, 특히 겨울에는 더욱 두꺼워져 혹한에도 잘 견딜 수 있도록 진화했습니다. 털 색깔은 주황색 바탕에 검은 줄무늬를 가지고 있는데, 다른 호랑이들보다 줄무늬가 옅고 간격이 넓은 편입니다.\n",
      "\n",
      "3.  **뛰어난 사냥 능력:** 육식 동물로, 주로 사슴, 멧돼지 등 비교적 큰 먹이를 사냥합니다. 뛰어난 청각, 시각, 후각을 이용하며, 강력한 턱과 발톱으로 먹이를 제압합니다. 매복 사냥에 능숙하며, 단독 생활을 합니다.\n",
      "\n",
      "4.  **넓은 서식 영역:** 단독 생활을 하며 매우 넓은 영역을 차지합니다. 주로 침엽수림이나 혼합림에서 서식하며, 영역 내에서 먹이를 찾고 번식합니다.\n",
      "\n",
      "5.  **강인함과 용맹함의 상징:** 한국 문화에서 호랑이는 산신령의 사자(使者)로 여겨지거나, 나쁜 기운을 물리치는 수호신, 그리고 강인함과 용맹함, 지혜를 상징하는 동물로 인식되어 왔습니다. 민화, 설화 등 다양한 예술과 이야기에 등장합니다.\n",
      "\n",
      "6.  **멸종 위기종:** 안타깝게도 시베리아 호랑이는 서식지 파괴와 밀렵 등으로 인해 현재 심각한 멸종 위기에 처해 있습니다. 전 세계적으로 야생에 남아있는 개체 수가 매우 적어 국제적인 보호 노력이 이루어지고 있습니다.\n",
      "\n",
      "이러한 특징들 때문에 백두산 호랑이는 단순히 동물을 넘어 한국인의 정신과 문화에 깊이 뿌리내린 상징적인 존재로 자리매김하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 대화 기록 생성\n",
    "messages = [\n",
    "    HumanMessage(content=\"안녕하세요! 저는 AI 튜터입니다. 무엇을 도와드릴까요?\"),\n",
    "    AIMessage(content=\"네, 반갑습니다! 저는 대한민국에 대해 궁금한 점이 많아요.\"),\n",
    "    HumanMessage(content=\"무엇이든 물어보세요. 예를 들어, 수도나 문화에 대해 궁금한가요?\"),\n",
    "]\n",
    "\n",
    "# 새로운 질문 추가\n",
    "messages.append(HumanMessage(content=\"대한민국의 상징적인 동물은 무엇인가요?\"))\n",
    "\n",
    "# Gemini 모델에 대화 전달\n",
    "chat_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"대화 기록:\")\n",
    "for msg in messages[:-1]: # 마지막 질문 제외\n",
    "    print(f\"  {msg.type}: {msg.content}\")\n",
    "print(f\"  새로운 질문: {messages[-1].content}\")\n",
    "print(f\"Gemini 답변: {chat_response.content}\")\n",
    "\n",
    "# 대화 이어가기\n",
    "messages.append(AIMessage(content=chat_response.content)) # 이전 답변을 대화 기록에 추가\n",
    "messages.append(HumanMessage(content=\"그 동물이 어떤 특징을 가지고 있나요?\"))\n",
    "\n",
    "chat_response_2 = llm.invoke(messages)\n",
    "\n",
    "print(f\"\\n추가 질문: {messages[-1].content}\")\n",
    "print(f\"Gemini 답변: {chat_response_2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507f5d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트: 'LangChain은 LLM 기반 애플리케이션 개발을 돕는 프레임워크입니다.'\n",
      "임베딩 벡터의 길이: 768\n",
      "\n",
      "여러 문서의 임베딩 생성 완료. 첫 번째 문서 임베딩 길이: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 임베딩 모델 초기화\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 단일 텍스트 임베딩 생성\n",
    "text_to_embed = \"LangChain은 LLM 기반 애플리케이션 개발을 돕는 프레임워크입니다.\"\n",
    "embedding = embeddings_model.embed_query(text_to_embed)\n",
    "\n",
    "print(f\"텍스트: '{text_to_embed}'\")\n",
    "print(f\"임베딩 벡터의 길이: {len(embedding)}\")\n",
    "# print(f\"임베딩 벡터의 일부: {embedding[:10]}...\") # 너무 길어서 일부만 출력\n",
    "\n",
    "# 여러 텍스트 임베딩 생성\n",
    "documents = [\n",
    "    \"저는 서울에 살고 있습니다.\",\n",
    "    \"대한민국의 수도는 서울입니다.\",\n",
    "    \"뉴욕은 미국의 한 도시입니다.\"\n",
    "]\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "print(f\"\\n여러 문서의 임베딩 생성 완료. 첫 번째 문서 임베딩 길이: {len(document_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca08e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 123과 456을 더하면 얼마인가요?\n",
      "Gemini의 응답 (도구 호출 정보 포함): [{'name': 'calculator', 'args': {'a': 123.0, 'b': 456.0}, 'id': 'f0c862dd-cd20-457d-bb69-495d8dcb5434', 'type': 'tool_call'}]\n",
      "도구 호출 결과: 579\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # 이 부분도 추가되어야 합니다.\n",
    "from langchain_core.messages import HumanMessage, AIMessage # 필요시 추가\n",
    "\n",
    "# 1. 사용할 도구 정의\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"첫 번째 숫자\")\n",
    "    b: int = Field(description=\"두 번째 숫자\")\n",
    "\n",
    "@tool(\"calculator\", args_schema=CalculatorInput)\n",
    "def calculator_tool(a: int, b: int) -> int:\n",
    "    \"\"\"두 숫자를 더하는 간단한 계산기\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 2. Gemini 모델에 도구 전달\n",
    "# tools=[calculator_tool]로 모델에 도구를 사용할 수 있음을 알립니다.\n",
    "# model=\"gemini-2.5-flash-preview-05-20\"는 현재 preview 버전이므로,\n",
    "# 나중에 안정화된 모델로 변경될 수 있습니다.\n",
    "llm_with_tools = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "llm_with_tools = llm_with_tools.bind_tools([calculator_tool])\n",
    "\n",
    "# 3. 모델에 질문하여 도구 사용 유도\n",
    "prompt_for_tool = \"123과 456을 더하면 얼마인가요?\"\n",
    "response_tool = llm_with_tools.invoke(prompt_for_tool)\n",
    "\n",
    "print(f\"질문: {prompt_for_tool}\")\n",
    "print(f\"Gemini의 응답 (도구 호출 정보 포함): {response_tool.tool_calls}\")\n",
    "\n",
    "# 실제 도구 호출 및 결과 처리 (이 부분은 애플리케이션 로직에 따라 달라짐)\n",
    "# 여기서는 응답에서 tool_calls가 있으면 직접 호출해 봅니다.\n",
    "if response_tool.tool_calls:\n",
    "    for tool_call in response_tool.tool_calls:\n",
    "        # 오류 수정: 딕셔너리 접근 방식으로 변경\n",
    "        if tool_call['name'] == \"calculator\":\n",
    "            result = calculator_tool.invoke(tool_call['args'])\n",
    "            print(f\"도구 호출 결과: {result}\")\n",
    "            # 이 결과를 다시 모델에게 전달하여 최종 답변을 받을 수 있습니다.\n",
    "            # (이 튜토리얼에서는 여기까지만 다룸)\n",
    "else:\n",
    "    print(f\"Gemini는 도구 호출 대신 직접 답변했습니다: {response_tool.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a505e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0710b03b",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f21b2f",
   "metadata": {},
   "source": [
    "LangChain의 체인(Chains)이란?\n",
    "\n",
    "LangChain에서 **체인(Chain)**은 여러 컴포넌트(LLM 호출, 프롬프트 템플릿, 파서, 다른 체인 등)를 결합하여 특정 작업을 수행하는 일련의 과정을 정의한 것입니다. 쉽게 말해, LLM 기반 애플리케이션의 워크플로우를 구성하는 도구라고 생각하시면 됩니다.\n",
    "\n",
    "단일 LLM 호출은 제한적일 수 있습니다. 예를 들어, 단순히 \"질문-답변\"만으로는 복잡한 문제를 해결하기 어렵습니다. 체인은 이러한 단일 LLM 호출의 한계를 극복하고, 여러 단계를 거쳐 더 정교하고 유용한 작업을 수행할 수 있도록 해줍니다.\n",
    "\n",
    "체인의 필요성 및 장점\n",
    "\n",
    "복잡한 작업 처리: 단일 LLM 호출로는 어려운 다단계 작업을 체인으로 정의할 수 있습니다. (예: 문서 로드 -> 분할 -> 임베딩 -> 벡터 저장소 저장 -> 검색 -> LLM 답변 생성)\n",
    "모듈화 및 재사용성: 각 컴포넌트(프롬프트, LLM, 파서 등)를 독립적으로 정의하고, 필요에 따라 다양한 체인에 재사용할 수 있습니다.\n",
    "오류 처리 및 로깅: 체인 내에서 각 단계의 성공/실패를 추적하고, 오류 발생 시 적절하게 처리할 수 있습니다.\n",
    "확장성: 새로운 컴포넌트를 추가하거나 기존 컴포넌트를 교체하여 기능을 쉽게 확장할 수 있습니다.\n",
    "명확한 워크플로우: 애플리케이션의 논리적 흐름을 시각적으로 또는 코드적으로 명확하게 파악할 수 있습니다.\n",
    "체인의 기본 구조 (일반적인 파이프라인)\n",
    "\n",
    "가장 기본적인 체인은 다음과 같은 파이프라인으로 구성될 수 있습니다:\n",
    "\n",
    "입력 (Input): 사용자로부터 받은 초기 데이터입니다.\n",
    "프롬프트 템플릿 (Prompt Template): 입력 데이터를 기반으로 LLM에게 보낼 프롬프트를 동적으로 생성합니다.\n",
    "LLM (Large Language Model): 생성된 프롬프트를 받아 텍스트를 생성합니다.\n",
    "출력 파서 (Output Parser): LLM의 원시 텍스트 출력을 구조화된 데이터(예: JSON, 리스트)로 변환합니다.\n",
    "출력 (Output): 최종 결과물입니다.\n",
    "코드 스니펫\n",
    "graph TD\n",
    "    A[입력] --> B[프롬프트 템플릿];\n",
    "    B --> C[LLM];\n",
    "    C --> D[출력 파서];\n",
    "    D --> E[출력];\n",
    "LangChain에서 체인 구현하기\n",
    "\n",
    "LangChain에서 체인을 구현하는 방법은 크게 두 가지가 있습니다:\n",
    "\n",
    "레거시 방식 (LLMChain, SequentialChain 등):\n",
    "\n",
    "이전 버전의 LangChain에서 주로 사용되던 방식입니다.\n",
    "특정 목적을 위한 미리 정의된 클래스들이 있습니다.\n",
    "LCEL (LangChain Expression Language):\n",
    "\n",
    "LangChain의 최신 권장 방식입니다.\n",
    "파이프(|) 연산자를 사용하여 컴포넌트들을 연결하는 유연하고 강력한 방법입니다.\n",
    "비동기 처리, 스트리밍, 병렬 처리 등 다양한 고급 기능을 쉽게 구현할 수 있습니다.\n",
    "이 방식을 주로 학습하시는 것을 강력히 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56086f",
   "metadata": {},
   "source": [
    "## 1. 기본 LLMChain (LCEL 방식) 예시\n",
    "\n",
    "가장 간단한 체인은 프롬프트와 LLM을 연결하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307486c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: LangChain에 대해 설명해주세요.\n",
      "답변: LLM 기반 복합 애플리케이션 개발 프레임워크. (20자)\n",
      "\n",
      "질문: 대한민국에 대해 설명해주세요.\n",
      "답변: 자유민주주의를 기반으로 첨단 기술과 한류를 선도하는 동아시아 국가. (29자)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. LLM 모델 초기화\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "\n",
    "# 2. 프롬프트 템플릿 정의\n",
    "# {topic}은 사용자 입력으로 대체될 변수입니다.\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}에 대해 30자 이내로 설명해주세요.\")\n",
    "\n",
    "# 3. 출력 파서 정의 (선택 사항이지만 유용)\n",
    "# LLM의 출력을 단순 문자열로 파싱합니다.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 구성 (LCEL 사용)\n",
    "# prompt | llm | output_parser: 'prompt'의 출력이 'llm'의 입력으로, 'llm'의 출력이 'output_parser'의 입력으로 전달됩니다.\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 5. 체인 실행\n",
    "# invoke 메서드에 프롬프트 템플릿의 변수에 해당하는 딕셔너리를 전달합니다.\n",
    "response = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(f\"질문: LangChain에 대해 설명해주세요.\")\n",
    "print(f\"답변: {response}\")\n",
    "\n",
    "response_korea = chain.invoke({\"topic\": \"대한민국\"})\n",
    "print(f\"\\n질문: 대한민국에 대해 설명해주세요.\")\n",
    "print(f\"답변: {response_korea}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962dad38",
   "metadata": {},
   "source": [
    "## 2. Sequential Chains (순차 체인) - LCEL 방식\n",
    "\n",
    "여러 체인을 순서대로 연결하여 복잡한 작업을 수행하는 경우입니다. 예를 들어, 먼저 입력 주제에 대한 아이디어를 생성하고, 그 아이디어로 시를 쓰는 체인을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05791302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주제: 여름밤\n",
      "--- 아이디어 생성 및 시 작성 시작 ---\n",
      "\n",
      "생성된 시:\n",
      "## 여름밤의 별빛 여정\n",
      "\n",
      "여름밤이 찾아오면, 도시의 숨소리 잠잠해지고\n",
      "별들이 속삭이는 시간, 특별한 마법이 시작돼.\n",
      "\n",
      "**첫째, 눈을 감고 귀 기울여봐요.**\n",
      "안대 너머, 풀벌레의 작은 합창\n",
      "바람이 나뭇잎 스치는 속삭임, 멀리 물결의 노래.\n",
      "시각이 멈춘 고요 속, 청각과 후각이 깨어나\n",
      "세상의 본질을 만나는 명상, 밤의 소리 탐험.\n",
      "오직 감각만이 살아 숨 쉬는, 진정한 몰입의 순간.\n",
      "\n",
      "**둘째, 손끝에서 피어나는 빛의 꿈.**\n",
      "재활용 병과 LED, 폐CD가 다시 태어나\n",
      "반짝이는 빛 조각으로 변신하는 마법.\n",
      "환경의 숨결 담아, 각자의 개성 담은 등불이\n",
      "밤의 캔버스 위에 반딧불이처럼 모여들어\n",
      "지상의 별자리, 환상의 숲을 이루네.\n",
      "아이들의 웃음과 함께, 빛나는 추억을 조각하는\n",
      "우리만의 야외 갤러리, '반딧불이의 꿈'.\n",
      "\n",
      "**셋째, 고개 들어 우주와 소풍을 떠나요.**\n",
      "도심을 벗어나 별이 쏟아지는 언덕 위,\n",
      "망원경 너머 행성과 은하의 아득한 속삭임.\n",
      "별자리 해설사의 목소리에 실린 오래된 신화들\n",
      "그리스 신들의 사랑과 비극, 동양의 전설들이\n",
      "밤하늘 아래 피크닉 담요 위에서 펼쳐져.\n",
      "간식을 나누며 인문학적 상상력에 잠기고\n",
      "디지털에서 벗어나 아날로그 감성으로\n",
      "우주와 교감하는, '별자리 이야기' 우주 소풍.\n",
      "\n",
      "소리로 듣고, 빛으로 만들고, 이야기로 채우는\n",
      "오감으로 깨어나는 여름밤의 특별함.\n",
      "일상의 굴레를 벗어던지고, 창의의 불꽃 피워\n",
      "우리는 모두 별이 되고, 꿈을 꾸는 반딧불이가 되네.\n",
      "여름밤의 마법 속에서, 진정한 나를 만나는 시간.\n",
      "이토록 찬란한 밤, 영원히 기억될 아름다운 여정.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# API 키 설정 (필수) | 이미 세팅되어 있음\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 첫 번째 체인: 주제에 대한 아이디어 생성\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic}에 대한 창의적인 아이디어를 3가지 제안해주세요.\")\n",
    "idea_chain = prompt1 | llm | output_parser\n",
    "\n",
    "# 두 번째 체인: 생성된 아이디어를 바탕으로 시 작성\n",
    "prompt2 = ChatPromptTemplate.from_template(\"다음 아이디어들을 참고하여 시를 작성해주세요:\\n\\n{ideas}\")\n",
    "poem_chain = prompt2 | llm | output_parser\n",
    "\n",
    "# 전체 순차 체인 구성 (LCEL의 RunnableParallel, RunnablePassthrough 사용)\n",
    "# input_chain: 'topic'을 받아서 'idea_chain'으로 전달하고,\n",
    "#             동시에 'topic' 자체도 다음 단계로 전달될 수 있도록 RunnablePassthrough 사용 (여기서는 불필요하지만 개념적으로)\n",
    "# pipe(|) 연산자를 통해 연결\n",
    "full_chain = (\n",
    "    {\"ideas\": idea_chain} # idea_chain의 결과가 \"ideas\" 키로 전달됩니다.\n",
    "    | poem_chain # poem_chain의 입력으로 \"ideas\"가 사용됩니다.\n",
    ")\n",
    "\n",
    "\n",
    "# 실행\n",
    "topic = \"여름밤\"\n",
    "print(f\"주제: {topic}\")\n",
    "print(\"--- 아이디어 생성 및 시 작성 시작 ---\")\n",
    "result = full_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"\\n생성된 시:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf32f2",
   "metadata": {},
   "source": [
    "## 3. Router Chains (라우터 체인) - LCEL 방식\n",
    "\n",
    "입력에 따라 동적으로 다른 체인이나 LLM을 선택하여 사용하는 고급 체인입니다. 특정 질문 유형에 따라 최적의 답변을 제공하는 데 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84c06dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 질문: 대한민국의 수도는 어디인가요? ---\n",
      "분류된 유형: fact\n",
      "답변: 대한민국의 수도는 **서울**입니다.\n",
      "\n",
      "--- 질문: 사랑이란 무엇인가요? ---\n",
      "분류된 유형: creative\n",
      "답변: 사랑이란, 정의할 수 없는 무한한 우주의 숨결이자, 우리 존재의 가장 깊은 곳에서 울려 퍼지는 신비로운 공명입니다.\n",
      "\n",
      "*   **별빛과 그림자:** 사랑은 어둠을 밝히는 가장 찬란한 별빛이면서 동시에, 그 빛이 드리우는 가장 깊은 그림자까지도 기꺼이 끌어안는 용기입니다. 기쁨의 절정에서 터져 나오는 웃음소리이자, 상실의 고통 속에서 흐르는 뜨거운 눈물이기도 합니다.\n",
      "\n",
      "*   **흐르는 강물과 뿌리 깊은 나무:** 사랑은 멈추지 않고 흐르는 강물처럼 끊임없이 변화하고 적응하며, 때로는 격렬하게, 때로는 고요하게 흘러갑니다. 동시에, 땅속 깊이 뿌리내린 나무처럼 흔들림 없는 안정감과 변치 않는 믿음을 제공합니다.\n",
      "\n",
      "*   **오케스트라와 침묵:** 사랑은 수많은 감정이 조화롭게 어우러진 웅장한 오케스트라입니다. 열정의 트럼펫, 연민의 바이올린, 슬픔의 첼로, 희망의 플루트가 한데 모여 교향곡을 연주합니다. 하지만 때로는 모든 소리가 멎은 고요한 침묵 속에서 가장 깊은 이해와 연결을 발견하게 하는 순간이기도 합니다.\n",
      "\n",
      "*   **숨 쉬는 예술 작품:** 사랑은 영원히 미완성인 예술 작품입니다. 매 순간 새로운 붓질이 더해지고, 새로운 색깔이 덧입혀지며, 때로는 과감하게 지워지고 다시 그려지기도 합니다. 두 영혼이 함께 만들어가는 유일무이한 걸작이며, 그 과정 자체가 가장 아름다운 행위입니다.\n",
      "\n",
      "*   **미지의 지도와 나침반:** 사랑은 정답이 없는 미지의 지도입니다. 어디로 이끌지 알 수 없지만, 두려움 없이 발을 내딛게 하는 용기를 줍니다. 그리고 그 길 위에서 헤맬 때, 내면의 가장 깊은 곳에서 길을 알려주는 유일한 나침반이 되어줍니다.\n",
      "\n",
      "*   **시간을 초월한 언어:** 사랑은 인류가 발명한 가장 오래되고 가장 보편적인 언어입니다. 말없이 눈빛으로, 작은 손짓으로, 따뜻한 온기로 모든 것을 전달할 수 있는, 시간을 초월하고 경계를 허무는 마법 같은 소통 방식입니다.\n",
      "\n",
      "결국 사랑은, 우리를 더 큰 존재로 만들고, 우리를 연결하며, 우리에게 삶의 가장 깊은 의미를 속삭여주는, 영원히 탐험해야 할 가장 아름다운 미스터리입니다. 그것은 단 하나의 정의로 가둘 수 없는, 살아 숨 쉬는 경험이자 끊임없이 진화하는 감정의 우주입니다.\n",
      "\n",
      "--- 질문: 지구는 왜 둥근가요? ---\n",
      "분류된 유형: fact\n",
      "답변: 지구가 둥근 주된 이유는 **중력** 때문입니다. 자세한 설명은 다음과 같습니다.\n",
      "\n",
      "1.  **중력의 균일한 당김:**\n",
      "    *   중력은 지구를 구성하는 모든 물질(암석, 물, 가스 등)을 지구의 질량 중심을 향해 끌어당기는 힘입니다.\n",
      "    *   이 힘은 모든 방향에서 균일하게 작용하며, 모든 물질이 중심으로부터 가장 가까운 거리에 있도록 배열하려는 경향이 있습니다.\n",
      "    *   이러한 배열의 결과로 형성되는 가장 효율적이고 안정적인 형태가 바로 **구(sphere)** 입니다.\n",
      "\n",
      "2.  **지구 형성 과정:**\n",
      "    *   지구가 처음 형성될 때, 우주의 먼지, 가스, 암석 등이 서로 충돌하고 합쳐지면서 거대한 덩어리를 이루었습니다. 이 과정에서 발생한 열로 인해 지구는 뜨겁고 녹은 상태(액체 상태)였습니다.\n",
      "    *   이 액체 상태에서 중력은 물질을 구형으로 재배열하기에 충분히 강력했으며, 지구가 점차 식고 굳어지면서 그 구형의 형태를 유지하게 되었습니다.\n",
      "\n",
      "3.  **완벽한 구형이 아닌 이유 (타원체):**\n",
      "    *   하지만 지구는 완벽한 구형은 아닙니다. 지구는 자전하기 때문에 **원심력**이 발생합니다.\n",
      "    *   이 원심력은 적도 부분에서 가장 강하게 작용하여, 적도 부분이 약간 부풀어 오르고 극 부분이 납작한 **타원체(oblate spheroid)** 형태를 띠고 있습니다. 즉, 적도 지름이 극 지름보다 약간 더 깁니다.\n",
      "    *   하지만 이 차이는 지구 전체 크기에 비하면 매우 미미한 수준이라, 멀리서 보면 거의 완벽한 구형으로 보입니다.\n",
      "\n",
      "4.  **지형의 영향 (미미함):**\n",
      "    *   산맥이나 해구 같은 지형의 높낮이는 지구 전체 크기(평균 반지름 약 6,371km)에 비하면 매우 미미한 수준입니다. 만약 지구를 당구공 크기로 줄인다면, 지구 표면은 당구공보다 훨씬 더 매끄러울 것입니다.\n",
      "\n",
      "결론적으로, 지구의 둥근 형태는 주로 **중력의 균일한 당김**과 지구 형성 과정에서 물질이 가장 효율적인 형태로 재배열된 결과이며, 자전으로 인해 약간 납작한 타원체 형태를 띠고 있습니다.\n",
      "\n",
      "--- 질문: 만약 동물이 말을 할 수 있다면 어떤 말을 할까요? ---\n",
      "분류된 유형: creative\n",
      "답변: 만약 동물이 말을 할 수 있다면, 이 세상은 지금과는 완전히 다른 소리와 의미로 가득 찰 것입니다. 그들의 언어는 인간의 언어보다 훨씬 원초적이고, 솔직하며, 때로는 예상치 못한 깊은 지혜를 담고 있을 거예요.\n",
      "\n",
      "몇 가지 상상해 보자면:\n",
      "\n",
      "1.  **개:** \"인간이여, 당신의 퇴근 시간은 왜 그리도 예측 불가능한가요? 당신이 문을 열고 들어올 때의 그 환희! 당신의 냄새는 이 세상에서 가장 아름다운 향수입니다. 그리고... 간식은 언제나 환영입니다. 당신이 슬플 때, 제가 곁에 있다는 걸 알아주세요. 저는 당신의 눈물 냄새를 맡을 수 있습니다.\"\n",
      "\n",
      "2.  **고양이:** \"인간? 당신은 그저 나의 편안한 침대이자, 식량 공급원, 그리고 가끔은 심심풀이 장난감일 뿐이다. 내 허락 없이 만지지 마라. 내 낮잠을 방해하지 마라. 그리고 왜 자꾸 내 발밑에 걸려 넘어지는가? 이 집은 나의 왕국이다. 너는 그저 나를 섬기는 하인일 뿐.\"\n",
      "\n",
      "3.  **새 (참새나 까마귀):** \"하늘은 누구의 것도 아니다. 너희의 경계선이 얼마나 우스운지 아느냐? 오늘 아침 햇살은 정말 완벽했어. 저기 저 인간은 또 뭘 흘렸군! 어서 가서 확인해 봐야지. 너희는 왜 그리 땅에만 매여 사는가? 위에서 보면 모든 것이 얼마나 작고 단순해 보이는지 모를 거다.\"\n",
      "\n",
      "4.  **물고기:** \"고요함의 미학을 아는가? 너희의 소음은 때로 너무 크다. 이 물은 내 집이자 숨결이다. 제발 더럽히지 마라. 이 작은 돌멩이 하나에도 우주의 순환이 담겨 있는데, 너희는 왜 그리 큰 것만 쫓는가? 나는 그저 흐름에 몸을 맡길 뿐.\"\n",
      "\n",
      "5.  **곰:** \"내 숲이다. 존중해라. 겨울잠은 삶의 순환이며, 꿀은 언제나 환영이다. 인간이여, 자연은 너희의 것이 아니다. 잠시 빌려 쓰는 것일 뿐. 너희의 욕심이 얼마나 많은 생명을 파괴하는지 아는가? 나는 그저 내 가족과 함께 평화롭게 살고 싶을 뿐이다.\"\n",
      "\n",
      "6.  **코끼리:** \"기억하라. 모든 발자국이 역사가 된다. 우리는 수천 년의 지혜를 발바닥으로 느끼고, 코로 숨 쉬며 살아간다. 가족은 전부이며, 슬픔은 함께 나누는 것이다. 이 땅은 우리 조상의 것이다. 너희의 짧은 삶으로 우리의 오랜 역사를 판단하지 마라.\"\n",
      "\n",
      "7.  **개미:** \"개인은 중요하지 않다. 우리는 전체다. 효율이 생존이다. 너희 인간은 너무 느리고 혼란스럽다. 왜 그리 많은 에너지를 불필요한 일에 낭비하는가? 우리에게는 단 하나의 목표가 있다: 군집의 번영. 너희는 너무나 많은 목표를 가지고 너무나 많은 싸움을 한다.\"\n",
      "\n",
      "동물들의 말은 아마 인간에게 많은 것을 가르쳐 줄 것입니다. 자연의 순리에 대한 겸손함, 단순한 삶의 기쁨, 그리고 우리가 얼마나 많은 것을 당연하게 여기고 살았는지에 대한 깨달음을 줄 수 있을 거예요. 그들의 언어는 우리에게 진정한 소통과 공감의 의미를 다시 생각하게 할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "import os\n",
    "\n",
    "# API 키 설정 (필수) | 이미 세팅되어 있음\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 서브 체인 1: 사실 질문 답변 체인\n",
    "fact_prompt = ChatPromptTemplate.from_template(\"다음 질문에 대해 사실에 기반하여 답변해주세요: {question}\")\n",
    "fact_chain = fact_prompt | llm | output_parser\n",
    "\n",
    "# 서브 체인 2: 창의적인 질문 답변 체인\n",
    "creative_prompt = ChatPromptTemplate.from_template(\"다음 질문에 대해 창의적이고 상상력을 발휘하여 답변해주세요: {question}\")\n",
    "creative_chain = creative_prompt | llm | output_parser\n",
    "\n",
    "# 라우터 프롬프트: 어떤 체인을 사용할지 LLM에게 물어봅니다.\n",
    "router_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "주어진 질문의 유형을 분류하세요: 'fact' (사실에 기반한 질문) 또는 'creative' (창의적인 질문).\n",
    "출력은 오직 'fact' 또는 'creative'여야 합니다.\n",
    "\n",
    "질문: {question}\n",
    "유형:\n",
    "\"\"\")\n",
    "\n",
    "# 라우터 체인: 라우터 프롬프트를 통해 유형을 파악하고, 그에 따라 다른 체인을 호출합니다.\n",
    "# RunnableBranch는 여러 조건을 검사하고, 조건에 맞는 Runnable을 실행합니다.\n",
    "# (조건, 실행될 Runnable) 튜플 리스트로 구성됩니다.\n",
    "router_chain = router_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 최종 라우팅 로직\n",
    "# RunnableBranch.from_default(default_runnable, (condition, runnable), ...)\n",
    "# 조건: router_chain을 통해 얻은 유형이 'fact'인지 'creative'인지 확인\n",
    "# runnable: 해당 유형일 경우 실행될 체인 (fact_chain 또는 creative_chain)\n",
    "# default_runnable: 어떤 조건에도 맞지 않을 경우 실행될 기본 체인\n",
    "final_chain = RunnableBranch(\n",
    "    (lambda x: \"fact\" in router_chain.invoke({\"question\": x[\"question\"]}).lower(), fact_chain),\n",
    "    (lambda x: \"creative\" in router_chain.invoke({\"question\": x[\"question\"]}).lower(), creative_chain),\n",
    "    fact_chain, # 기본값 (어떤 조건에도 맞지 않으면 fact_chain으로)\n",
    ")\n",
    "\n",
    "# 입력은 RunnablePassthrough를 통해 모든 체인에 전달될 수 있도록 합니다.\n",
    "# 실제로는 router_chain이 먼저 실행되어 라우팅 결정을 하고, 그 결과를 바탕으로 final_chain이 선택됩니다.\n",
    "# 이 예시는 간단화를 위해 라우터 체인을 두 번 호출하는 형태가 되었지만, 실제로는 한번의 분류로 처리합니다.\n",
    "# 더 효율적인 구현은 라우터 체인의 출력을 파싱하여 분기를 직접 제어하는 방식이 될 것입니다.\n",
    "\n",
    "# LCEL에서 라우터를 구현하는 더 일반적인 방식은 이렇게 구성될 수 있습니다:\n",
    "# router = RunnableBranch(\n",
    "#     (lambda x: \"fact\" in x[\"topic_type\"], fact_chain),\n",
    "#     (lambda x: \"creative\" in x[\"topic_type\"], creative_chain),\n",
    "#     fact_chain # Default\n",
    "# )\n",
    "#\n",
    "# full_router_chain = {\n",
    "#     \"topic_type\": router_chain, # 라우터 체인 실행 결과 (fact/creative)\n",
    "#     \"question\": RunnablePassthrough() # 원래 질문도 다음으로 전달\n",
    "# } | router\n",
    "\n",
    "\n",
    "# 실제 사용 예시 (간단화를 위해 라우터 체인 분리 실행)\n",
    "def route_and_invoke(question):\n",
    "    print(f\"\\n--- 질문: {question} ---\")\n",
    "    # 라우터 모델을 호출하여 질문 유형 분류\n",
    "    question_type = router_chain.invoke({\"question\": question}).strip().lower()\n",
    "    print(f\"분류된 유형: {question_type}\")\n",
    "\n",
    "    if \"fact\" in question_type:\n",
    "        result = fact_chain.invoke({\"question\": question})\n",
    "    elif \"creative\" in question_type:\n",
    "        result = creative_chain.invoke({\"question\": question})\n",
    "    else:\n",
    "        result = fact_chain.invoke({\"question\": question}) # 기본값\n",
    "    print(f\"답변: {result}\")\n",
    "\n",
    "route_and_invoke(\"대한민국의 수도는 어디인가요?\")\n",
    "route_and_invoke(\"사랑이란 무엇인가요?\")\n",
    "route_and_invoke(\"지구는 왜 둥근가요?\")\n",
    "route_and_invoke(\"만약 동물이 말을 할 수 있다면 어떤 말을 할까요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f97ae",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4b9ee",
   "metadata": {},
   "source": [
    "RAG는 LLM(대규모 언어 모델) 기반 애플리케이션 개발에서 가장 중요하고 강력한 패턴 중 하나입니다. LLM의 기본적인 지식 한계(학습 데이터 시점 이후의 정보 부족, 환각 현상 등)를 극복하고, 최신 정보나 특정 도메인 지식을 활용하여 답변의 정확성과 신뢰성을 높이는 방법입니다.\n",
    "\n",
    "**RAG가 중요한 이유**\n",
    "\n",
    "1. 정보 최신성: LLM은 학습된 데이터 시점까지만 정보를 알고 있습니다. RAG는 외부의 최신 데이터베이스나 문서를 참조하여 항상 최신 정보로 답변할 수 있게 합니다.\n",
    "\n",
    "2. 환각(Hallucination) 감소: LLM이 없는 내용을 지어내는 현상(환각)을 줄여줍니다. 실제 데이터에 기반하여 답변하므로 신뢰성이 높아집니다.\n",
    "\n",
    "3. 특정 도메인 지식 활용: 기업 내부 문서, 특정 전문 분야의 논문 등 LLM이 학습하지 않은 고유한 지식을 활용하여 답변할 수 있게 합니다.\n",
    "\n",
    "4. 근거 제시: 답변의 출처를 명확히 제시하여 사용자가 답변의 정확성을 검증할 수 있게 합니다.\n",
    "\n",
    "**RAG 시스템의 작동 원리 (간략)**\n",
    "\n",
    "기본적인 RAG 시스템은 다음과 같은 단계를 거칩니다:\n",
    "\n",
    "1. 문서 로드 (Document Loading): PDF, 웹페이지, 텍스트 파일 등 다양한 형식의 원본 데이터를 불러옵니다.\n",
    "\n",
    "2. 문서 분할 (Text Splitting): 긴 문서를 LLM이 처리할 수 있는 적절한 크기의 작은 '청크(chunk)'로 나눕니다.\n",
    "\n",
    "3. 임베딩 생성 (Embeddings): 분할된 각 청크를 숫자 벡터(임베딩)로 변환합니다. 유사한 의미를 가진 텍스트는 유사한 벡터 공간에 위치하게 됩니다.\n",
    "\n",
    "4. 벡터 저장소 저장 (Vector Store): 생성된 임베딩과 원본 텍스트 청크를 **벡터 데이터베이스 (Vector Store)**에 저장합니다.\n",
    "\n",
    "5. 질의 임베딩 (Query Embedding): 사용자의 질문이 들어오면, 이 질문 또한 임베딩 벡터로 변환합니다.\n",
    "\n",
    "6. 유사성 검색 (Similarity Search): 질의 임베딩과 벡터 저장소에 저장된 문서 임베딩 간의 유사성을 측정하여, 질문과 가장 관련성이 높은 문서 청크들을 검색합니다.\n",
    "\n",
    "7. LLM 답변 생성 (LLM Generation): 검색된 관련 문서 청크들과 사용자의 원본 질문을 LLM에게 함께 전달(컨텍스트로 제공)하여 최종 답변을 생성하게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e53713",
   "metadata": {},
   "source": [
    "## RAG 시스템 구축: 1단계 - 문서 로드 (Document Loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f9e0cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로드된 문서 개수: 1\n",
      "\n",
      "--- 첫 번째 문서 내용 (page_content) ---\n",
      "대한민국은 동아시아에 위치한 아름다운 나라입니다.\n",
      "수도는 서울이며, 첨단 기술과 전통 문화가 공존하는 활기찬 도시입니다.\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는...\n",
      "\n",
      "--- 첫 번째 문서 메타데이터 (metadata) ---\n",
      "{'source': 'my_document.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader 초기화 및 파일 경로 지정\n",
    "loader = TextLoader(\"my_document.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# 문서 로드 (load() 메서드는 Document 객체 리스트를 반환합니다)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"로드된 문서 개수: {len(documents)}\")\n",
    "\n",
    "# 첫 번째 문서의 내용 확인\n",
    "if documents:\n",
    "    first_doc = documents[0]\n",
    "    print(\"\\n--- 첫 번째 문서 내용 (page_content) ---\")\n",
    "    print(first_doc.page_content[:100] + \"...\") # 내용이 길면 일부만 출력\n",
    "\n",
    "    print(\"\\n--- 첫 번째 문서 메타데이터 (metadata) ---\")\n",
    "    print(first_doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e55348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서 개수: 1\n",
      "분할된 청크 개수: 5\n",
      "\n",
      "--- 분할된 청크 내용 ---\n",
      "청크 1 (시작 인덱스: 0, 길이: 27):\n",
      "대한민국은 동아시아에 위치한 아름다운 나라입니다.\n",
      "--------------------\n",
      "청크 2 (시작 인덱스: 28, 길이: 39):\n",
      "수도는 서울이며, 첨단 기술과 전통 문화가 공존하는 활기찬 도시입니다.\n",
      "--------------------\n",
      "청크 3 (시작 인덱스: 68, 길이: 47):\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는 과학적인 문자 체계입니다.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. 이전 단계에서 로드했던 문서 사용\n",
    "loader = TextLoader(\"my_document.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. RecursiveCharacterTextSplitter 초기화\n",
    "# chunk_size: 각 청크의 최대 토큰(문자) 수 (예시를 위해 작게 설정)\n",
    "# chunk_overlap: 인접한 청크 간의 중복 문자 수\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,       # 50글자 (또는 토큰) 단위로 분할\n",
    "    chunk_overlap=10,    # 10글자 겹치게\n",
    "    length_function=len, # 길이를 계산하는 함수 (기본값은 len)\n",
    "    add_start_index=True # 각 청크의 원본 문서 시작 인덱스 추가 (메타데이터로)\n",
    ")\n",
    "\n",
    "# 3. 문서 분할 실행\n",
    "# split_documents() 메서드는 Document 객체 리스트를 반환합니다.\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"원본 문서 개수: {len(documents)}\")\n",
    "print(f\"분할된 청크 개수: {len(chunks)}\")\n",
    "\n",
    "# 분할된 청크 내용 확인\n",
    "print(\"\\n--- 분할된 청크 내용 ---\")\n",
    "for i, chunk in enumerate(chunks[:3]): # 처음 3개 청크만 확인\n",
    "    print(f\"청크 {i+1} (시작 인덱스: {chunk.metadata.get('start_index', 'N/A')}, 길이: {len(chunk.page_content)}):\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# 모든 청크의 메타데이터 확인 (선택 사항)\n",
    "# for chunk in chunks:\n",
    "#     print(chunk.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a5b15",
   "metadata": {},
   "source": [
    "## RAG 시스템 구축: 3단계 - 임베딩 생성 (Embeddings Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32950dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 청크 개수: 5\n",
      "생성된 임베딩 벡터 개수: 5\n",
      "\n",
      "--- 첫 번째 청크 임베딩 벡터의 길이: 768\n",
      "--- 첫 번째 청크 임베딩 벡터의 일부 (처음 5개): [0.05413832142949104, 0.0059584276750683784, -0.05648026987910271, -0.017137177288532257, 0.058552488684654236]...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # 임베딩 모델 임포트\n",
    "import os\n",
    "\n",
    "# 1. API 키 설정 (이전과 동일하게 설정되어 있어야 합니다)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# 2. 이전 단계에서 로드하고 분할했던 문서 청크 사용\n",
    "loader = TextLoader(\"my_document.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Gemini 임베딩 모델 초기화\n",
    "# model=\"models/embedding-001\"은 Gemini에서 제공하는 임베딩 모델입니다.\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 4. 청크 임베딩 생성\n",
    "# embed_documents() 메서드는 Document 객체 리스트를 받아 각 문서의 임베딩 벡터 리스트를 반환합니다.\n",
    "# 각 벡터는 해당 Document의 page_content에 대한 임베딩입니다.\n",
    "embedded_chunks = embeddings_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "\n",
    "print(f\"원본 청크 개수: {len(chunks)}\")\n",
    "print(f\"생성된 임베딩 벡터 개수: {len(embedded_chunks)}\")\n",
    "\n",
    "# 첫 번째 청크의 임베딩 벡터 확인 (길이와 일부 값)\n",
    "if embedded_chunks:\n",
    "    first_embedding = embedded_chunks[0]\n",
    "    print(f\"\\n--- 첫 번째 청크 임베딩 벡터의 길이: {len(first_embedding)}\")\n",
    "    print(f\"--- 첫 번째 청크 임베딩 벡터의 일부 (처음 5개): {first_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28868dcd",
   "metadata": {},
   "source": [
    "## RAG 시스템 구축: 4단계 - 벡터 저장소 (Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a573e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma 벡터 저장소가 성공적으로 구축되었습니다!\n",
      "문서 청크와 임베딩이 저장되었습니다.\n",
      "Chroma 데이터가 저장된 디렉터리: /Users/hyunjong/Desktop/KHJ/personal/gemini_dev/chroma_db\n",
      "디렉터리 내용: ['e3addcf5-f247-4e95-a672-db991d9f3557', 'chroma.sqlite3']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma # Chroma 벡터 저장소 임포트\n",
    "import os\n",
    "\n",
    "# 1. API 키 설정 (필수)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# 2. 이전 단계에서 로드하고 분할했던 문서 청크 사용\n",
    "loader = TextLoader(\"my_document.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Gemini 임베딩 모델 초기화 (이전과 동일)\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 4. Chroma 벡터 저장소에 문서 청크와 임베딩 저장\n",
    "# from_documents() 메서드는 Document 객체 리스트와 임베딩 모델을 받아 벡터 저장소를 구축합니다.\n",
    "# persist_directory: 데이터를 저장할 디렉터리 (선택 사항, 지정하지 않으면 인메모리)\n",
    "#                 이 디렉터리를 지정하면 다음번에 프로그램을 실행할 때 다시 임베딩을 생성할 필요 없이\n",
    "#                 저장된 벡터 저장소를 불러올 수 있습니다.\n",
    "#                   Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings_model)\n",
    "#                   이렇게도 직접 초기화 가능\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\" # 데이터를 저장할 로컬 디렉터리 (선택 사항)\n",
    ")\n",
    "\n",
    "print(\"Chroma 벡터 저장소가 성공적으로 구축되었습니다!\")\n",
    "print(\"문서 청크와 임베딩이 저장되었습니다.\")\n",
    "\n",
    "# 저장된 벡터 저장소 파일 확인\n",
    "# 지정한 './chroma_db' 디렉터리에 파일들이 생성되었을 것입니다.\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    print(f\"Chroma 데이터가 저장된 디렉터리: {os.path.abspath('./chroma_db')}\")\n",
    "    print(f\"디렉터리 내용: {os.listdir('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46e26f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 Chroma 벡터 저장소를 성공적으로 불러왔습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dx/ppdw271s0mbdmkwsgbyv70t00000gn/T/ipykernel_43235/2271799451.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  loaded_vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# 기존에 저장된 벡터 저장소를 불러올 때\n",
    "# (임베딩 모델은 동일하게 초기화되어 있어야 합니다)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "loaded_vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings_model # 불러올 때도 임베딩 함수를 지정해야 합니다.\n",
    ")\n",
    "\n",
    "print(\"기존 Chroma 벡터 저장소를 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3583a6",
   "metadata": {},
   "source": [
    "## RAG 시스템 구축: 5단계 - 리트리버 (Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab2fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma 벡터 저장소가 준비되었습니다.\n",
      "리트리버가 생성되었습니다.\n",
      "\n",
      "--- 질의: '한국 문화와 관련된 내용은 뭔가요?' ---\n",
      "--- 검색된 문서 (2개) ---\n",
      "\n",
      "문서 1 (출처: my_document.txt, 시작 인덱스: 68):\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는 과학적인 문자 체계입니다.\n",
      "------------------------------\n",
      "\n",
      "문서 2 (출처: my_document.txt, 시작 인덱스: 68):\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는 과학적인 문자 체계입니다.\n",
      "------------------------------\n",
      "\n",
      "--- 질의: '대한민국의 수도는 어디야?' ---\n",
      "--- 검색된 문서 (2개) ---\n",
      "\n",
      "문서 1 (출처: my_document.txt, 시작 인덱스: 68):\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는 과학적인 문자 체계입니다.\n",
      "------------------------------\n",
      "\n",
      "문서 2 (출처: my_document.txt, 시작 인덱스: 68):\n",
      "한국어는 대한민국의 공용어이며, 한글은 세계적으로 인정받는 과학적인 문자 체계입니다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "# 1. API 키 설정 (필수)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# 2. 이전 단계에서 로드, 분할, 저장했던 벡터 저장소 로드 또는 재구축\n",
    "# 실제 사용 시에는 이전에 저장된 파일을 로드하는 것이 효율적입니다.\n",
    "# 하지만 튜토리얼의 연속성을 위해 여기서는 다시 구축하는 코드를 포함합니다.\n",
    "\n",
    "# 문서 로드\n",
    "loader = TextLoader(\"my_document.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 모델 초기화\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Chroma 벡터 저장소 구축 또는 로드\n",
    "# persist_directory가 지정되어 있으므로, 이미 데이터가 있다면 불러오고, 없다면 새로 생성합니다.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"Chroma 벡터 저장소가 준비되었습니다.\")\n",
    "\n",
    "# 3. 벡터 저장소에서 리트리버 생성\n",
    "# as_retriever() 메서드를 사용하여 Retriever 객체를 얻습니다.\n",
    "# search_kwargs={\"k\": 2}는 검색할 문서 청크의 개수를 지정합니다 (상위 2개).\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(\"리트리버가 생성되었습니다.\")\n",
    "\n",
    "# 4. 리트리버를 사용하여 관련 문서 검색\n",
    "query = \"한국 문화와 관련된 내용은 뭔가요?\"\n",
    "retrieved_docs = retriever.invoke(query) # invoke 메서드를 사용하여 쿼리 수행\n",
    "\n",
    "print(f\"\\n--- 질의: '{query}' ---\")\n",
    "print(f\"--- 검색된 문서 ({len(retrieved_docs)}개) ---\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n문서 {i+1} (출처: {doc.metadata.get('source', 'N/A')}, 시작 인덱스: {doc.metadata.get('start_index', 'N/A')}):\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "query_capital = \"대한민국의 수도는 어디야?\"\n",
    "retrieved_docs_capital = retriever.invoke(query_capital)\n",
    "\n",
    "print(f\"\\n--- 질의: '{query_capital}' ---\")\n",
    "print(f\"--- 검색된 문서 ({len(retrieved_docs_capital)}개) ---\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs_capital):\n",
    "    print(f\"\\n문서 {i+1} (출처: {doc.metadata.get('source', 'N/A')}, 시작 인덱스: {doc.metadata.get('start_index', 'N/A')}):\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352eb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

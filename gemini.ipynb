{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09a1efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY가 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# API 키 확인 (선택 사항)\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    print(\"GOOGLE_API_KEY가 설정되지 않았습니다.\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY가 설정되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d7ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini 모델이 성공적으로 초기화되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Gemini 모델 초기화\n",
    "# model_name은 \"gemini-pro\" 또는 \"gemini-1.5-pro-latest\" 등을 사용할 수 있습니다.\n",
    "# temperature는 생성될 텍스트의 무작위성(창의성)을 조절합니다. 0에 가까울수록 보수적입니다.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "\n",
    "print(\"Gemini 모델이 성공적으로 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d828f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 한국의 수도는 어디인가요?\n",
      "답변: 대한민국의 수도는 **서울**입니다.\n",
      "\n",
      "질문: 한국의 수도 서울에 대해 50자 이내로 설명해주세요.\n",
      "답변: 한국의 수도 서울은 역사와 현대가 공존하는 활기찬 대도시입니다. (28자)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 단일 질문\n",
    "prompt = \"한국의 수도는 어디인가요?\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(f\"질문: {prompt}\")\n",
    "print(f\"답변: {response.content}\")\n",
    "\n",
    "# 조금 더 복잡한 질문\n",
    "prompt_complex = \"한국의 수도 서울에 대해 50자 이내로 설명해주세요.\"\n",
    "response_complex = llm.invoke(prompt_complex)\n",
    "print(f\"\\n질문: {prompt_complex}\")\n",
    "print(f\"답변: {response_complex.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a023e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 기록:\n",
      "  human: 안녕하세요! 저는 AI 튜터입니다. 무엇을 도와드릴까요?\n",
      "  ai: 네, 반갑습니다! 저는 대한민국에 대해 궁금한 점이 많아요.\n",
      "  human: 무엇이든 물어보세요. 예를 들어, 수도나 문화에 대해 궁금한가요?\n",
      "  새로운 질문: 대한민국의 상징적인 동물은 무엇인가요?\n",
      "Gemini 답변: 네, 아주 좋은 질문입니다!\n",
      "\n",
      "대한민국의 상징적인 동물은 바로 **호랑이**입니다.\n",
      "\n",
      "특히 **백두산 호랑이**는 한국인의 기상과 용맹함을 상징하며, 예로부터 한반도의 지형을 호랑이 모양으로 비유하기도 했습니다. 한국 민화나 설화에도 호랑이가 자주 등장하며, 산신령의 사자(使者)로 여겨지기도 하는 등 한국 문화와 깊이 연관되어 있습니다.\n",
      "\n",
      "2018 평창 동계올림픽 마스코트인 '수호랑'도 호랑이를 모티브로 한 것이었죠.\n",
      "\n",
      "궁금증이 해결되셨기를 바랍니다! 또 다른 질문이 있으신가요?\n",
      "\n",
      "추가 질문: 그 동물이 어떤 특징을 가지고 있나요?\n",
      "Gemini 답변: 네, 대한민국의 상징적인 동물인 **호랑이**, 특히 **백두산 호랑이 (시베리아 호랑이)**는 다음과 같은 특징들을 가지고 있습니다.\n",
      "\n",
      "1.  **가장 큰 호랑이 아종:** 시베리아 호랑이는 현존하는 호랑이 아종 중에서 몸집이 가장 큽니다. 수컷은 몸길이 2.5~3미터(꼬리 포함), 몸무게 200~300kg에 달하기도 합니다. 암컷은 그보다 작습니다.\n",
      "\n",
      "2.  **두껍고 긴 털:** 추운 북방 지역에 서식하기 때문에 다른 호랑이 아종에 비해 털이 길고 빽빽하며, 특히 겨울에는 더욱 두꺼워져 혹한에도 잘 견딜 수 있도록 진화했습니다. 털 색깔은 주황색 바탕에 검은 줄무늬를 가지고 있는데, 다른 호랑이들보다 줄무늬가 옅고 간격이 넓은 편입니다.\n",
      "\n",
      "3.  **뛰어난 사냥 능력:** 육식 동물로, 주로 사슴, 멧돼지 등 비교적 큰 먹이를 사냥합니다. 뛰어난 청각, 시각, 후각을 이용하며, 강력한 턱과 발톱으로 먹이를 제압합니다. 매복 사냥에 능숙하며, 단독 생활을 합니다.\n",
      "\n",
      "4.  **넓은 서식 영역:** 단독 생활을 하며 매우 넓은 영역을 차지합니다. 주로 침엽수림이나 혼합림에서 서식하며, 영역 내에서 먹이를 찾고 번식합니다.\n",
      "\n",
      "5.  **강인함과 용맹함의 상징:** 한국 문화에서 호랑이는 산신령의 사자(使者)로 여겨지거나, 나쁜 기운을 물리치는 수호신, 그리고 강인함과 용맹함, 지혜를 상징하는 동물로 인식되어 왔습니다. 민화, 설화 등 다양한 예술과 이야기에 등장합니다.\n",
      "\n",
      "6.  **멸종 위기종:** 안타깝게도 시베리아 호랑이는 서식지 파괴와 밀렵 등으로 인해 현재 심각한 멸종 위기에 처해 있습니다. 전 세계적으로 야생에 남아있는 개체 수가 매우 적어 국제적인 보호 노력이 이루어지고 있습니다.\n",
      "\n",
      "이러한 특징들 때문에 백두산 호랑이는 단순히 동물을 넘어 한국인의 정신과 문화에 깊이 뿌리내린 상징적인 존재로 자리매김하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 대화 기록 생성\n",
    "messages = [\n",
    "    HumanMessage(content=\"안녕하세요! 저는 AI 튜터입니다. 무엇을 도와드릴까요?\"),\n",
    "    AIMessage(content=\"네, 반갑습니다! 저는 대한민국에 대해 궁금한 점이 많아요.\"),\n",
    "    HumanMessage(content=\"무엇이든 물어보세요. 예를 들어, 수도나 문화에 대해 궁금한가요?\"),\n",
    "]\n",
    "\n",
    "# 새로운 질문 추가\n",
    "messages.append(HumanMessage(content=\"대한민국의 상징적인 동물은 무엇인가요?\"))\n",
    "\n",
    "# Gemini 모델에 대화 전달\n",
    "chat_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"대화 기록:\")\n",
    "for msg in messages[:-1]: # 마지막 질문 제외\n",
    "    print(f\"  {msg.type}: {msg.content}\")\n",
    "print(f\"  새로운 질문: {messages[-1].content}\")\n",
    "print(f\"Gemini 답변: {chat_response.content}\")\n",
    "\n",
    "# 대화 이어가기\n",
    "messages.append(AIMessage(content=chat_response.content)) # 이전 답변을 대화 기록에 추가\n",
    "messages.append(HumanMessage(content=\"그 동물이 어떤 특징을 가지고 있나요?\"))\n",
    "\n",
    "chat_response_2 = llm.invoke(messages)\n",
    "\n",
    "print(f\"\\n추가 질문: {messages[-1].content}\")\n",
    "print(f\"Gemini 답변: {chat_response_2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507f5d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트: 'LangChain은 LLM 기반 애플리케이션 개발을 돕는 프레임워크입니다.'\n",
      "임베딩 벡터의 길이: 768\n",
      "\n",
      "여러 문서의 임베딩 생성 완료. 첫 번째 문서 임베딩 길이: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 임베딩 모델 초기화\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 단일 텍스트 임베딩 생성\n",
    "text_to_embed = \"LangChain은 LLM 기반 애플리케이션 개발을 돕는 프레임워크입니다.\"\n",
    "embedding = embeddings_model.embed_query(text_to_embed)\n",
    "\n",
    "print(f\"텍스트: '{text_to_embed}'\")\n",
    "print(f\"임베딩 벡터의 길이: {len(embedding)}\")\n",
    "# print(f\"임베딩 벡터의 일부: {embedding[:10]}...\") # 너무 길어서 일부만 출력\n",
    "\n",
    "# 여러 텍스트 임베딩 생성\n",
    "documents = [\n",
    "    \"저는 서울에 살고 있습니다.\",\n",
    "    \"대한민국의 수도는 서울입니다.\",\n",
    "    \"뉴욕은 미국의 한 도시입니다.\"\n",
    "]\n",
    "document_embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "print(f\"\\n여러 문서의 임베딩 생성 완료. 첫 번째 문서 임베딩 길이: {len(document_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca08e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 123과 456을 더하면 얼마인가요?\n",
      "Gemini의 응답 (도구 호출 정보 포함): [{'name': 'calculator', 'args': {'a': 123.0, 'b': 456.0}, 'id': 'f0c862dd-cd20-457d-bb69-495d8dcb5434', 'type': 'tool_call'}]\n",
      "도구 호출 결과: 579\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # 이 부분도 추가되어야 합니다.\n",
    "from langchain_core.messages import HumanMessage, AIMessage # 필요시 추가\n",
    "\n",
    "# 1. 사용할 도구 정의\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"첫 번째 숫자\")\n",
    "    b: int = Field(description=\"두 번째 숫자\")\n",
    "\n",
    "@tool(\"calculator\", args_schema=CalculatorInput)\n",
    "def calculator_tool(a: int, b: int) -> int:\n",
    "    \"\"\"두 숫자를 더하는 간단한 계산기\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 2. Gemini 모델에 도구 전달\n",
    "# tools=[calculator_tool]로 모델에 도구를 사용할 수 있음을 알립니다.\n",
    "# model=\"gemini-2.5-flash-preview-05-20\"는 현재 preview 버전이므로,\n",
    "# 나중에 안정화된 모델로 변경될 수 있습니다.\n",
    "llm_with_tools = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "llm_with_tools = llm_with_tools.bind_tools([calculator_tool])\n",
    "\n",
    "# 3. 모델에 질문하여 도구 사용 유도\n",
    "prompt_for_tool = \"123과 456을 더하면 얼마인가요?\"\n",
    "response_tool = llm_with_tools.invoke(prompt_for_tool)\n",
    "\n",
    "print(f\"질문: {prompt_for_tool}\")\n",
    "print(f\"Gemini의 응답 (도구 호출 정보 포함): {response_tool.tool_calls}\")\n",
    "\n",
    "# 실제 도구 호출 및 결과 처리 (이 부분은 애플리케이션 로직에 따라 달라짐)\n",
    "# 여기서는 응답에서 tool_calls가 있으면 직접 호출해 봅니다.\n",
    "if response_tool.tool_calls:\n",
    "    for tool_call in response_tool.tool_calls:\n",
    "        # 오류 수정: 딕셔너리 접근 방식으로 변경\n",
    "        if tool_call['name'] == \"calculator\":\n",
    "            result = calculator_tool.invoke(tool_call['args'])\n",
    "            print(f\"도구 호출 결과: {result}\")\n",
    "            # 이 결과를 다시 모델에게 전달하여 최종 답변을 받을 수 있습니다.\n",
    "            # (이 튜토리얼에서는 여기까지만 다룸)\n",
    "else:\n",
    "    print(f\"Gemini는 도구 호출 대신 직접 답변했습니다: {response_tool.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a505e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0710b03b",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f21b2f",
   "metadata": {},
   "source": [
    "LangChain의 체인(Chains)이란?\n",
    "\n",
    "LangChain에서 **체인(Chain)**은 여러 컴포넌트(LLM 호출, 프롬프트 템플릿, 파서, 다른 체인 등)를 결합하여 특정 작업을 수행하는 일련의 과정을 정의한 것입니다. 쉽게 말해, LLM 기반 애플리케이션의 워크플로우를 구성하는 도구라고 생각하시면 됩니다.\n",
    "\n",
    "단일 LLM 호출은 제한적일 수 있습니다. 예를 들어, 단순히 \"질문-답변\"만으로는 복잡한 문제를 해결하기 어렵습니다. 체인은 이러한 단일 LLM 호출의 한계를 극복하고, 여러 단계를 거쳐 더 정교하고 유용한 작업을 수행할 수 있도록 해줍니다.\n",
    "\n",
    "체인의 필요성 및 장점\n",
    "\n",
    "복잡한 작업 처리: 단일 LLM 호출로는 어려운 다단계 작업을 체인으로 정의할 수 있습니다. (예: 문서 로드 -> 분할 -> 임베딩 -> 벡터 저장소 저장 -> 검색 -> LLM 답변 생성)\n",
    "모듈화 및 재사용성: 각 컴포넌트(프롬프트, LLM, 파서 등)를 독립적으로 정의하고, 필요에 따라 다양한 체인에 재사용할 수 있습니다.\n",
    "오류 처리 및 로깅: 체인 내에서 각 단계의 성공/실패를 추적하고, 오류 발생 시 적절하게 처리할 수 있습니다.\n",
    "확장성: 새로운 컴포넌트를 추가하거나 기존 컴포넌트를 교체하여 기능을 쉽게 확장할 수 있습니다.\n",
    "명확한 워크플로우: 애플리케이션의 논리적 흐름을 시각적으로 또는 코드적으로 명확하게 파악할 수 있습니다.\n",
    "체인의 기본 구조 (일반적인 파이프라인)\n",
    "\n",
    "가장 기본적인 체인은 다음과 같은 파이프라인으로 구성될 수 있습니다:\n",
    "\n",
    "입력 (Input): 사용자로부터 받은 초기 데이터입니다.\n",
    "프롬프트 템플릿 (Prompt Template): 입력 데이터를 기반으로 LLM에게 보낼 프롬프트를 동적으로 생성합니다.\n",
    "LLM (Large Language Model): 생성된 프롬프트를 받아 텍스트를 생성합니다.\n",
    "출력 파서 (Output Parser): LLM의 원시 텍스트 출력을 구조화된 데이터(예: JSON, 리스트)로 변환합니다.\n",
    "출력 (Output): 최종 결과물입니다.\n",
    "코드 스니펫\n",
    "graph TD\n",
    "    A[입력] --> B[프롬프트 템플릿];\n",
    "    B --> C[LLM];\n",
    "    C --> D[출력 파서];\n",
    "    D --> E[출력];\n",
    "LangChain에서 체인 구현하기\n",
    "\n",
    "LangChain에서 체인을 구현하는 방법은 크게 두 가지가 있습니다:\n",
    "\n",
    "레거시 방식 (LLMChain, SequentialChain 등):\n",
    "\n",
    "이전 버전의 LangChain에서 주로 사용되던 방식입니다.\n",
    "특정 목적을 위한 미리 정의된 클래스들이 있습니다.\n",
    "LCEL (LangChain Expression Language):\n",
    "\n",
    "LangChain의 최신 권장 방식입니다.\n",
    "파이프(|) 연산자를 사용하여 컴포넌트들을 연결하는 유연하고 강력한 방법입니다.\n",
    "비동기 처리, 스트리밍, 병렬 처리 등 다양한 고급 기능을 쉽게 구현할 수 있습니다.\n",
    "이 방식을 주로 학습하시는 것을 강력히 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56086f",
   "metadata": {},
   "source": [
    "## 1. 기본 LLMChain (LCEL 방식) 예시\n",
    "\n",
    "가장 간단한 체인은 프롬프트와 LLM을 연결하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307486c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: LangChain에 대해 설명해주세요.\n",
      "답변: LLM 기반 복합 애플리케이션 개발 프레임워크. (20자)\n",
      "\n",
      "질문: 대한민국에 대해 설명해주세요.\n",
      "답변: 자유민주주의를 기반으로 첨단 기술과 한류를 선도하는 동아시아 국가. (29자)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. LLM 모델 초기화\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "\n",
    "# 2. 프롬프트 템플릿 정의\n",
    "# {topic}은 사용자 입력으로 대체될 변수입니다.\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}에 대해 30자 이내로 설명해주세요.\")\n",
    "\n",
    "# 3. 출력 파서 정의 (선택 사항이지만 유용)\n",
    "# LLM의 출력을 단순 문자열로 파싱합니다.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. 체인 구성 (LCEL 사용)\n",
    "# prompt | llm | output_parser: 'prompt'의 출력이 'llm'의 입력으로, 'llm'의 출력이 'output_parser'의 입력으로 전달됩니다.\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 5. 체인 실행\n",
    "# invoke 메서드에 프롬프트 템플릿의 변수에 해당하는 딕셔너리를 전달합니다.\n",
    "response = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(f\"질문: LangChain에 대해 설명해주세요.\")\n",
    "print(f\"답변: {response}\")\n",
    "\n",
    "response_korea = chain.invoke({\"topic\": \"대한민국\"})\n",
    "print(f\"\\n질문: 대한민국에 대해 설명해주세요.\")\n",
    "print(f\"답변: {response_korea}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962dad38",
   "metadata": {},
   "source": [
    "## 2. Sequential Chains (순차 체인) - LCEL 방식\n",
    "\n",
    "여러 체인을 순서대로 연결하여 복잡한 작업을 수행하는 경우입니다. 예를 들어, 먼저 입력 주제에 대한 아이디어를 생성하고, 그 아이디어로 시를 쓰는 체인을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05791302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주제: 여름밤\n",
      "--- 아이디어 생성 및 시 작성 시작 ---\n",
      "\n",
      "생성된 시:\n",
      "## 여름밤의 별빛 여정\n",
      "\n",
      "여름밤이 찾아오면, 도시의 숨소리 잠잠해지고\n",
      "별들이 속삭이는 시간, 특별한 마법이 시작돼.\n",
      "\n",
      "**첫째, 눈을 감고 귀 기울여봐요.**\n",
      "안대 너머, 풀벌레의 작은 합창\n",
      "바람이 나뭇잎 스치는 속삭임, 멀리 물결의 노래.\n",
      "시각이 멈춘 고요 속, 청각과 후각이 깨어나\n",
      "세상의 본질을 만나는 명상, 밤의 소리 탐험.\n",
      "오직 감각만이 살아 숨 쉬는, 진정한 몰입의 순간.\n",
      "\n",
      "**둘째, 손끝에서 피어나는 빛의 꿈.**\n",
      "재활용 병과 LED, 폐CD가 다시 태어나\n",
      "반짝이는 빛 조각으로 변신하는 마법.\n",
      "환경의 숨결 담아, 각자의 개성 담은 등불이\n",
      "밤의 캔버스 위에 반딧불이처럼 모여들어\n",
      "지상의 별자리, 환상의 숲을 이루네.\n",
      "아이들의 웃음과 함께, 빛나는 추억을 조각하는\n",
      "우리만의 야외 갤러리, '반딧불이의 꿈'.\n",
      "\n",
      "**셋째, 고개 들어 우주와 소풍을 떠나요.**\n",
      "도심을 벗어나 별이 쏟아지는 언덕 위,\n",
      "망원경 너머 행성과 은하의 아득한 속삭임.\n",
      "별자리 해설사의 목소리에 실린 오래된 신화들\n",
      "그리스 신들의 사랑과 비극, 동양의 전설들이\n",
      "밤하늘 아래 피크닉 담요 위에서 펼쳐져.\n",
      "간식을 나누며 인문학적 상상력에 잠기고\n",
      "디지털에서 벗어나 아날로그 감성으로\n",
      "우주와 교감하는, '별자리 이야기' 우주 소풍.\n",
      "\n",
      "소리로 듣고, 빛으로 만들고, 이야기로 채우는\n",
      "오감으로 깨어나는 여름밤의 특별함.\n",
      "일상의 굴레를 벗어던지고, 창의의 불꽃 피워\n",
      "우리는 모두 별이 되고, 꿈을 꾸는 반딧불이가 되네.\n",
      "여름밤의 마법 속에서, 진정한 나를 만나는 시간.\n",
      "이토록 찬란한 밤, 영원히 기억될 아름다운 여정.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# API 키 설정 (필수) | 이미 세팅되어 있음\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0.7)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 첫 번째 체인: 주제에 대한 아이디어 생성\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic}에 대한 창의적인 아이디어를 3가지 제안해주세요.\")\n",
    "idea_chain = prompt1 | llm | output_parser\n",
    "\n",
    "# 두 번째 체인: 생성된 아이디어를 바탕으로 시 작성\n",
    "prompt2 = ChatPromptTemplate.from_template(\"다음 아이디어들을 참고하여 시를 작성해주세요:\\n\\n{ideas}\")\n",
    "poem_chain = prompt2 | llm | output_parser\n",
    "\n",
    "# 전체 순차 체인 구성 (LCEL의 RunnableParallel, RunnablePassthrough 사용)\n",
    "# input_chain: 'topic'을 받아서 'idea_chain'으로 전달하고,\n",
    "#             동시에 'topic' 자체도 다음 단계로 전달될 수 있도록 RunnablePassthrough 사용 (여기서는 불필요하지만 개념적으로)\n",
    "# pipe(|) 연산자를 통해 연결\n",
    "full_chain = (\n",
    "    {\"ideas\": idea_chain} # idea_chain의 결과가 \"ideas\" 키로 전달됩니다.\n",
    "    | poem_chain # poem_chain의 입력으로 \"ideas\"가 사용됩니다.\n",
    ")\n",
    "\n",
    "\n",
    "# 실행\n",
    "topic = \"여름밤\"\n",
    "print(f\"주제: {topic}\")\n",
    "print(\"--- 아이디어 생성 및 시 작성 시작 ---\")\n",
    "result = full_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"\\n생성된 시:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c06dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
